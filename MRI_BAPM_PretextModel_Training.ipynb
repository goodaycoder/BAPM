{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edfe0e95-06d4-4487-b45e-290abac2c555",
   "metadata": {},
   "source": [
    "<a href=\"https://mingxia.web.unc.edu/\" target=\"_parent\"><img src=\"https://mingxia.web.unc.edu/wp-content/uploads/sites/12411/2020/12/logo_MagicLab-horizontal-4.png\" alt=\"MAGIC Lab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22493e6f-93c8-41c9-91d1-6e23191a7e81",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **BAPM Pretext Model Training**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ca9df7-5fd3-4c39-b21d-2874094f870e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Loading required libraries**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f754556f-f4f9-4c5e-a62c-832c7879fb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "import sys, argparse\n",
    "import enum\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "import multiprocessing\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import pylab as pl\n",
    "import logging\n",
    "import shutil\n",
    "import tempfile\n",
    "import gzip\n",
    "from typing import Optional, Sequence, Tuple, Union\n",
    "from urllib.request import urlretrieve\n",
    "from PIL import Image\n",
    "\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from IPython import display\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "import copy\n",
    "import pprint\n",
    "import torchio as tio\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.nn import L1Loss\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix, roc_auc_score, matthews_corrcoef\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import svm\n",
    "\n",
    "from neuroCombat import neuroCombat\n",
    "\n",
    "import monai\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.data import CacheDataset, DataLoader, Dataset, ImageDataset\n",
    "from monai.networks.nets import VarAutoEncoder,ViTAutoEnc, AutoEncoder\n",
    "from monai.networks.layers.convutils import calculate_out_shape, same_padding\n",
    "from monai.networks.layers.factories import Act, Norm\n",
    "from monai.networks.utils import one_hot\n",
    "from monai.utils import set_determinism, first\n",
    "from monai.utils.enums import MetricReduction\n",
    "from monai.metrics import compute_hausdorff_distance, HausdorffDistanceMetric\n",
    "from monai.losses import ContrastiveLoss, DiceLoss, DiceCELoss\n",
    "from monai.transforms import (\n",
    "    ConvertToMultiChannelBasedOnBratsClasses,\n",
    "    AsDiscrete,\n",
    "    Activations,\n",
    "    AddChannelD,\n",
    "    Compose,\n",
    "    LoadImageD,\n",
    "    ScaleIntensityD,\n",
    "    EnsureTypeD,\n",
    "    LoadImaged,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    CopyItemsd,\n",
    "    SpatialPadd,\n",
    "    EnsureChannelFirstd,\n",
    "    Spacingd,\n",
    "    OneOf,\n",
    "    ScaleIntensityRanged,\n",
    "    RandSpatialCropSamplesd,\n",
    "    RandCoarseDropoutd,\n",
    "    RandCoarseShuffled\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0295062b-9ce3-40c4-b301-05c4a668aeb2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Global Setting**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "945f383c-0a8a-4d77-96d8-8e369159da1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "set_determinism(seed=0)\n",
    "\n",
    "pin_memory = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "im_shape = (1,176,208,176)\n",
    "modelname = 'AE2DecPriorV1'\n",
    "train_size = 0.8\n",
    "val_interval = 5\n",
    "pretrained = False\n",
    "downsampled = False\n",
    "samplespace = 1\n",
    "if downsampled:\n",
    "    samplespace = 2\n",
    "pretrained_path = './pretrained/'\n",
    "trained_path = './models/'\n",
    "logdir_path = os.path.normpath('./log/')\n",
    "if os.path.exists(logdir_path)==False:\n",
    "    os.mkdir(logdir_path)\n",
    "if os.path.exists(pretrained_path)==False:\n",
    "    os.mkdir(pretrained_path)\n",
    "if os.path.exists(trained_path)==False:\n",
    "    os.mkdir(trained_path)    \n",
    "\n",
    "if pretrained:\n",
    "    savedir = trained_path+'Pretrained_'\n",
    "else:\n",
    "    savedir = trained_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5b2035-122a-480f-af93-cbc105ff3fe6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**File scanner**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43efa064-bf26-44b4-b482-bb5ec3c577fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScanFile(object):\n",
    "    def __init__(self, directory, prefix=None, postfix=None):\n",
    "        self.directory = directory\n",
    "        self.prefix = prefix\n",
    "        self.postfix = postfix\n",
    "\n",
    "    def scan_files(self):\n",
    "        files_list = []\n",
    "\n",
    "        for dirpath, dirnames, filenames in os.walk(self.directory):\n",
    "            ''''' \n",
    "            dirpath is a string, the path to the directory.   \n",
    "            dirnames is a list of the names of the subdirectories in dirpath (excluding '.' and '..'). \n",
    "            filenames is a list of the names of the non-directory files in dirpath. \n",
    "            '''\n",
    "            for special_file in filenames:\n",
    "                if self.postfix:\n",
    "                    if special_file.endswith(self.postfix):\n",
    "                        files_list.append(os.path.join(dirpath, special_file))\n",
    "                elif self.prefix:\n",
    "                    if special_file.startswith(self.prefix):\n",
    "                        files_list.append(os.path.join(dirpath, special_file))\n",
    "                else:\n",
    "                    files_list.append(os.path.join(dirpath, special_file))\n",
    "\n",
    "        return files_list\n",
    "\n",
    "    def scan_subdir(self):\n",
    "        subdir_list = []\n",
    "        for dirpath, dirnames, files in os.walk(self.directory):\n",
    "            subdir_list.append(dirpath)\n",
    "        return subdir_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a55753-a937-46cd-b616-6ef27ee9d0d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Preparing for data reading**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c879bb58-1bdc-43a0-8cc0-898f194455d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNI_dir   = './data/ADNI_iBEAT_linearReg/'\n",
    "scan1 = ScanFile(ADNI_dir, postfix='n3.nii.gz')\n",
    "scan2 = ScanFile(ADNI_dir, postfix='-seg.nii.gz')\n",
    "\n",
    "ADNI_mri  = sorted(scan1.scan_files())\n",
    "ADNI_seg  = sorted(scan2.scan_files())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47ec1cb-37b6-4e93-bb73-b65769a00a09",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Define Dataset with Augamentation**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6206502-1531-40f8-b1f6-8ff7193b622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDD_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, segs):\n",
    "        subjects = []            \n",
    "        for (image, seg) in zip(images, segs):\n",
    "            subject = tio.Subject(\n",
    "                mri=tio.ScalarImage(image),\n",
    "                seg=tio.LabelMap(seg),\n",
    "            )\n",
    "            subjects.append(subject)\n",
    "        self.transform()\n",
    "        self.dataset = tio.SubjectsDataset(subjects, transform=self.aug_transform)\n",
    "            \n",
    "    def transform(self):\n",
    "        MDD_FSL_t1_mpr_landmarks       = np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 19.15165923, 43.90807752, 75.49163173, 100.])\n",
    "        MDD_FSL_t1_gre_landmarks       = np.array([0., 0.38107146, 0.38107149, 0.38107149, 0.38107149, 0.38107149, 0.38107149, 0.3810715, 0.64192136, 48.8195904, 71.07814235, 89.23094854, 100.])\n",
    "        MDD_FSL_t1_uchc_landmarks      = np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 39.38873128, 52.8757862, 71.36841272, 100.])\n",
    "        landmarks_dict1 = {'mri': MDD_FSL_t1_mpr_landmarks}\n",
    "        landmarks_dict2 = {'mri': MDD_FSL_t1_gre_landmarks}\n",
    "        landmarks_dict3 = {'mri': MDD_FSL_t1_uchc_landmarks}\n",
    "        prior_augment = tio.Compose([\n",
    "            #tio.ToCanonical(),\n",
    "            tio.CropOrPad((176, 208, 176)),                                              # tight crop around brain\n",
    "            tio.OneOf({                                # either\n",
    "                tio.HistogramStandardization(landmarks_dict1),\n",
    "                tio.HistogramStandardization(landmarks_dict2),\n",
    "                tio.HistogramStandardization(landmarks_dict3),\n",
    "            }),                                   \n",
    "            tio.OneOf({                                # either\n",
    "                tio.RandomBlur(std = (3,3,3)):1.0,                    # blur 25% of times\n",
    "                tio.RandomNoise(std=6):1.0,            # Gaussian noise 25% of times\n",
    "                tio.RandomBiasField():1.0,                # magnetic field inhomogeneity 30% of times\n",
    "                tio.RandomMotion(degrees = 2, translation = 0, num_transforms = 2): 1.0,    # random motion artifact\n",
    "            }),                                   \n",
    "            tio.RescaleIntensity(percentiles=(0.5,99.5), out_min_max=(0, 1.0)),\n",
    "        ])\n",
    "\n",
    "        self.aug_transform = prior_augment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36e95d5-433f-47e9-8b54-a36dbd25e640",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Data Loader**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c240491-23f7-42ac-9ed3-d5f4944a1c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(imagepaths,segpaths, batch_size=1):\n",
    "    dataset = MDD_Dataset(images=imagepaths,segs=segpaths)\n",
    "    loader = DataLoader(dataset.dataset,batch_size=batch_size,pin_memory=pin_memory)\n",
    "    return loader\n",
    "\n",
    "test_number =  200\n",
    "train_number = int((len(ADNI_mri)-test_number)*train_size)-1\n",
    "#print('train_number:'+str(train_number))\n",
    "train_MRI_loader = get_loader(ADNI_mri[0:train_number],ADNI_seg[0:train_number], batch_size=2)\n",
    "#test_loader  = get_loader(ADNI_mri[-test_number:],ADNI_seg[-test_number:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c570fe-94d1-4cc5-994c-6d6942d5c1d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Model definition**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee26fdd8-0e60-45d8-a502-73ef46cf7b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE2DecoderV1(AutoEncoder):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spatial_dims: int,\n",
    "        in_shape: Sequence[int],\n",
    "        out_channels: int,\n",
    "        out_channels2: int,\n",
    "        channels: Sequence[int],\n",
    "        strides: Sequence[int],\n",
    "        kernel_size: Union[Sequence[int], int] = 3,\n",
    "        up_kernel_size: Union[Sequence[int], int] = 3,\n",
    "        num_res_units: int = 0,\n",
    "        inter_channels: Optional[list] = None,\n",
    "        inter_dilations: Optional[list] = None,\n",
    "        num_inter_units: int = 2,\n",
    "        act: Optional[Union[Tuple, str]] = Act.PRELU,\n",
    "        norm: Union[Tuple, str] = Norm.INSTANCE,\n",
    "        dropout: Optional[Union[Tuple, str, float]] = None,\n",
    "        bias: bool = True,\n",
    "    ) -> None:\n",
    "\n",
    "        self.in_channels, *self.in_shape = in_shape\n",
    "        self.final_size = np.asarray(self.in_shape, dtype=int)\n",
    "\n",
    "        super().__init__(\n",
    "            spatial_dims,\n",
    "            self.in_channels,\n",
    "            out_channels,\n",
    "            channels,\n",
    "            strides,\n",
    "            kernel_size,\n",
    "            up_kernel_size,\n",
    "            num_res_units,\n",
    "            inter_channels,\n",
    "            inter_dilations,\n",
    "            num_inter_units,\n",
    "            act,\n",
    "            norm,\n",
    "            dropout,\n",
    "            bias,\n",
    "        )\n",
    "\n",
    "        padding = same_padding(self.kernel_size)\n",
    "\n",
    "        for s in strides:\n",
    "            self.final_size = calculate_out_shape(self.final_size, self.kernel_size, s, padding)  # type: ignore\n",
    "\n",
    "        linear_size = int(np.product(self.final_size)) * self.encoded_channels\n",
    "\n",
    "        decode_channel_list1 = list(channels[-2::-1]) + [out_channels]\n",
    "        self.decode1, _ = self._get_decode_module(self.encoded_channels//2, decode_channel_list1, strides[::-1] or [1])\n",
    "        decode_channel_list2 = list(channels[-2::-1]) + [out_channels2]\n",
    "        self.decode2, _ = self._get_decode_module(self.encoded_channels//2, decode_channel_list2, strides[::-1] or [1])\n",
    "\n",
    "\n",
    "    def decode_seg(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.decode2(x)\n",
    "        x = torch.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.encode(x)\n",
    "        feature = self.intermediate(x)\n",
    "        feature1, feature2 = torch.chunk(feature,2,dim=1)\n",
    "        mri  = self.decode1(feature1)\n",
    "        seg  = self.decode2(feature2)\n",
    "        return mri, seg\n",
    "\n",
    "model_AE2Dec = AE2DecoderV1(\n",
    "        spatial_dims=3,\n",
    "        in_shape=im_shape,\n",
    "        out_channels=1,#mri\n",
    "        out_channels2=4,#seg\n",
    "        channels=(64,128,256,512),\n",
    "#        channels=(16,32,32,64),\n",
    "        strides=(2,2,2,2),\n",
    "#        inter_channels=(64, 64),\n",
    "        inter_channels=(512,512*2),\n",
    "#        inter_dilations=(1, 2),\n",
    "        inter_dilations=(1, 1),\n",
    "        num_inter_units=2,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984f2fe6-9bdc-42ab-a37f-01bbab88c237",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Training Function**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b8e63da-4f8d-421d-aef5-7d9fc7e5e115",
   "metadata": {},
   "outputs": [],
   "source": [
    "L1Loss  = torch.nn.L1Loss(reduction='sum')\n",
    "DiceLoss = DiceLoss(to_onehot_y=True, softmax=True)\n",
    "\n",
    "def train(model, max_epochs, learning_rate, modelname):\n",
    "    model.to(device)\n",
    "    avg_train_losses = []\n",
    "    avg_train_dice_losses =[]\n",
    "    avg_train_mse_losses = []\n",
    "    avg_train_l1_losses = []\n",
    "    test_losses = []\n",
    "    t = trange(max_epochs, leave=True, desc=\"step: 0,  epoch: 0,   average train loss: ?, test loss: ?\")\n",
    "    bestloss = sys.maxsize\n",
    "\n",
    "    for epoch in t:\n",
    "        model.train()\n",
    "        mse_losses = []\n",
    "        dice_losses = []\n",
    "        l1_losses = []\n",
    "        epoch_losses = []\n",
    "        epoch_loss = 0\n",
    "        l1_loss = 0\n",
    "        dice_loss = 0\n",
    "        step = 0\n",
    "        for batch_data in train_MRI_loader:\n",
    "            step +=1\n",
    "            seg    = batch_data['seg'][tio.DATA].to(device).float()\n",
    "            target = inputs = batch_data['mri'][tio.DATA].to(device).float()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            recon_mri, recon_seg = model(inputs)\n",
    " \n",
    "            diceLoss = 100*DiceLoss(recon_seg, seg)\n",
    "            L1_Loss = 10000*L1Loss(recon_mri, target)/(176*208*176)\n",
    "            loss = L1_Loss + diceLoss\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            dice_losses.append(diceLoss.item())\n",
    "            l1_losses.append(L1_Loss.item())\n",
    "            epoch_losses.append(loss.item())\n",
    "            t.set_description(f\"step: {step}, epoch: {epoch + 1}\")\n",
    "        scheduler.step()\n",
    "        epoch_loss = np.mean(epoch_losses)\n",
    "        dice_loss  = np.mean(dice_losses)\n",
    "        l1_loss    = np.mean(l1_losses)\n",
    "        avg_train_losses.append(epoch_loss)\n",
    "        avg_train_dice_losses.append(dice_loss)\n",
    "        avg_train_l1_losses.append(l1_loss)\n",
    "\n",
    "        if (epoch+1)%val_interval == 0 and (epoch+1)> 0 and epoch_loss < bestloss:\n",
    "            bestloss = epoch_loss\n",
    "            AE_model_name = savedir+modelname +f\"_mri_seg_epoch{25+epoch+1}_diceloss{dice_loss:.2f}_l1loss{l1_loss:.2f}.pth\"\n",
    "            print(AE_model_name)\n",
    "            torch.save(model.state_dict(), AE_model_name)\n",
    "\n",
    "        if len(avg_train_losses)>0:\n",
    "            t.set_postfix(avg_train_losses=avg_train_losses[-1], avg_train_dice_losses=avg_train_dice_losses[-1], avg_train_l1_losses=avg_train_l1_losses[-1])\n",
    "                                   \n",
    "    return model, avg_train_losses, avg_train_dice_losses, avg_train_l1_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5151e11c-e67f-4f4d-8ad5-852f4ea1e3ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Training Process**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9a3bb1e-01c1-4dea-a86c-a2e19c8aaade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step: 4, epoch: 1:   0%|          | 0/10 [00:31<?, ?it/s]                                        \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_611023/1628291704.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_train_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_train_dice_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_train_l1_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_trainsize'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodelname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_mri_l1_seg_dice'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_epoch'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_611023/1807507332.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, max_epochs, learning_rate, modelname)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mL1_Loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mL1Loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_mri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m176\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m208\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m176\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL1_Loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdiceLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torch/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torch/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_epochs = 10\n",
    "learning_rate = 1e-4\n",
    "pretrained = False\n",
    "preAE_model = './models/XXX.pth'\n",
    "model = model_AE2Dec\n",
    "if pretrained:\n",
    "    print('loading: '+preAE_model)\n",
    "    model.load_state_dict(torch.load(preAE_model), strict = True)\n",
    "        \n",
    "optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "model, avg_train_losses, avg_train_dice_losses, avg_train_l1_losses = train(model, max_epochs, learning_rate, modelname+'_trainsize'+str(train_size))\n",
    "\n",
    "modelname = modelname+'_mri_l1_seg_dice'+'_epoch'+str(max_epochs)\n",
    "AE_model_name = savedir+modelname+'.pth'\n",
    "print(AE_model_name)\n",
    "torch.save(model.state_dict(), AE_model_name)\n",
    "print(\"Training Finish!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
